---
layout: default
permalink: /blogs/2025-10-3-gui/
title: "The Evolving Face of GUI (II) – GUI Grounding"
date: 2025-10-03
excerpt: "Some popular benchmarks and algorithms for GUI grounding."
tags: [gui, attention, visual grounding]
author_profile: true

---

<div class="blog-post" markdown="1">

# The Evolving Face of GUI (II) – GUI Grounding

## 1. Preface: "Basic" Math Formula

How to calculate rewards from each click better? Will GUI-R1 be faced with a sparse reward problem? Well here are some distribution formula from [GUI-G2](https://arxiv.org/pdf/2507.15846) to overcome it:

### Gaussian Point Rewards

The point reward evaluates localization precision by measuring how well the predicted center aligns with the target element's Gaussian distribution. Given a predicted bounding box with center $\mu_p = (c_x^p, c_y^p)$ and ground truth center $\mu_{gt} = (c_x^{gt}, c_y^{gt})$, we compute:

$$R_{point} = \mathcal{N}(\mu_p; \mu_{gt}, \Sigma_{gt}) = \exp\left(-\frac{1}{2}\left(\frac{(c_x^p - c_x^{gt})^2}{\sigma_x^2} + \frac{(c_y^p - c_y^{gt})^2}{\sigma_y^2}\right)\right) \quad (2)$$

This formulation provides several key properties. First, the reward reaches its maximum value of 1 when the predicted center perfectly aligns with the ground truth. Second, it decreases smoothly and exponentially with distance, ensuring continuous gradients throughout the spatial domain. Third, the rate of decay is controlled by the variance parameters ($\sigma_x^2, \sigma_y^2$), allowing flexible adaptation to different element characteristics.

### Gaussian Coverage Rewards

While point rewards optimize for center alignment, GUI interactions often succeed when clicking anywhere within element boundaries. Coverage rewards capture this regional aspect by measuring the spatial overlap between predicted and target Gaussian distributions. We quantify this overlap using the Bhattacharyya coefficient:

$$BC(\mathcal{N}_p, \mathcal{N}_{gt}) = \int \sqrt{\mathcal{N}(x; \mu_p, \Sigma_p) \cdot \mathcal{N}(x; \mu_{gt}, \Sigma_{gt})} \ dx \quad (3)$$

For Gaussian distributions, this integral has a closed-form solution:

$$R_{coverage} = \exp\left(-\frac{1}{8}(\mu_p - \mu_{gt})^\top \Sigma^{-1}(\mu_p - \mu_{gt}) - \frac{1}{2}\ln\left(\frac{\det(\Sigma)}{\sqrt{\det(\Sigma_p)\det(\Sigma_{gt})}}\right)\right) \quad (4)$$

where $\Sigma = \frac{\Sigma_p + \Sigma_{gt}}{2}$ is the average covariance. The first term penalizes center misalignment weighted by the combined uncertainty, while the second term measures size and shape similarity between distributions.

---

## 2. Datasets and Benchmarks

### Dataset: Scalebility vs Efficiency

When it comes to teaching AI how to understand user interfaces, **UGround** is the dataset equivalent of a well-organized desktop (which, let’s be honest, none of us actually have). Released in 2024 by researchers at Ohio State University, UGround is a large-scale dataset designed to make models see GUIs the way humans do — not through the safety net of DOM trees or accessibility tags, but directly through pixels.

UGround features over **1.3 million screenshots** spanning mobile, desktop, and web platforms, paired with roughly 10 million referring expressions (think of instructions like “click the tiny gear on the top right” or “open the second Settings tab”). These expressions link natural language to the correct on-screen element, enabling models to learn visual grounding from the screen alone.

What makes UGround fun — and slightly chaotic — is that it pushes beyond synthetic UIs or HTML metadata. It deals with real, messy user interfaces where buttons hide, icons look alike, and text labels disappear. This realism helps grounding models survive outside the lab, where not every “Submit” button actually says “Submit.”

Then there comes **GUI-R1**. By leveraging a small but meticulously curated set of high-quality data across multiple platforms — including Windows, Linux, macOS, Android, and Web — and employing policy optimization algorithms such as **Group Relative Policy Optimization (GRPO)** for model updates, GUI-R1 achieves remarkable efficiency. It reaches superior performance using **only 0.02%** of the data (3K vs. 13M) compared to previous state-of-the-art systems like OS-Atlas, while outperforming them across eight major GUI benchmarks covering mobile, desktop, and web environments. These results highlight the enormous potential of reinforcement learning based on unified action space rule modeling for improving the execution capabilities of large vision-language models (LVLMs) in real-world GUI agent tasks.

### Benchmark: How Grounded Are You, Really?

Okay, we’ve got data. But how do we grade a GUI agent’s eyesight? Cue the parade of benchmarks.

ScreenSpot (Cheng et al., 2024) was one of the first big names in GUI grounding evaluation. It offered a large-scale benchmark for matching textual descriptions to UI elements across web and mobile screens. Its improved descendants — ScreenSpot-v2 (Wu et al., 2024b) and ScreenSpot-Pro (Li et al., 2025) — progressively leveled up realism: fewer cropped screenshots, more cluttered layouts, smaller icons, **higher resolution** and more human-like referring expressions. In short, your model now has to find the right button in the wild, not in a tutorial sandbox.

Then came UI-I2E-Bench (Liu et al., 2025) — short for Instruction-to-Element. It’s like ScreenSpot after a few philosophy classes: instead of simple direct commands ("click OK"), it tests models with vague or indirect language ("approve the current dialog"). This forces the model to reason about intent, not just word-to-label matches. It’s where grounding meets commonsense.

Meanwhile, UI-Vision (Nayak et al., 2025) expanded the playing field with multi-scale and multi-type grounding. It includes everything from buttons to sliders to multi-element widgets. This benchmark challenges models to handle not just visual diversity but functional variety — the difference between “click this icon” and “drag this slider halfway.”

Finally, the grand integrator: MMBench-GUI (Wang et al., 2025). Think of it as the Boss Level of GUI grounding evaluations. MMBench-GUI organizes its tasks into four hierarchical levels:

Level 1 (L1) – Screen Understanding: recognizing what’s on the interface.

Level 2 (L2) – Element Grounding: pinpointing the exact GUI element from language (our main interest).

Level 3 (L3) – Task Execution: performing the required operation.

Level 4 (L4) – Cross-App Collaboration: orchestrating multi-step, multi-application tasks.

Level-2 (L2) is where grounding truly shines — it’s the layer that tests whether your model can point to the right thing when given an instruction. Miss here, and everything above collapses like a bad Jenga tower.

---

## 3. Representative works

| Model             | Category                 | Method  | Year |
|--------------------|--------------------------|----------------------|------|
| GPT-4o             | Proprietary Model        | --                   | 2024 |
| Claude Computer Use| Proprietary Model        | --                   | 2024 |
| Qwen2.5-VL-3B      | General Open-source Model| --                   | 2024 |
| Qwen2.5-VL-7B      | General Open-source Model| --                   | 2024 |
| SeeClick-9.6B      | GUI-specific Model (SFT) | --                   | 2024 |
| FOCUS-2B           | GUI-specific Model (SFT) | --                   | 2024 |
| CogAgent-18B       | GUI-specific Model (SFT) | --                   | 2024 |
| Aria-UI            | GUI-specific Model (SFT) | --                   | 2024 |
| OS-Atlas-7B        | GUI-specific Model (SFT) | --                   | 2024 |
| ShowUI-2B          | GUI-specific Model (SFT) | --                   | 2024 |
| UGround-7B         | GUI-specific Model (SFT) | --                   | 2025 |
| UGround-V1-7B      | GUI-specific Model (SFT) | --                   | 2025 |
| UI-TARS-2B         | GUI-specific Model (SFT) | --                   | 2025 |
| UI-TARS-7B         | GUI-specific Model (SFT) | --                   | 2025 |
| UI-TARS-72B        | GUI-specific Model (SFT) | --                   | 2025 |
| JEDI-3B            | GUI-specific Model (SFT) | --                   | 2025 |
| JEDI-7B            | GUI-specific Model (SFT) | --                   | 2025 |
| GUI-Actor-7B       | GUI-specific Model (SFT) | Attention-based                | 2025 |
| V2P-7B             | GUI-specific Model (SFT) | Attention-based                | 2025 |
| UI-R1-3B           | GUI-specific Model (RL)  | --                   | 2025 |
| UI-R1-E-3B         | GUI-specific Model (RL)  | --                   | 2025 |
| GUI-R1-3B          | GUI-specific Model (RL)  | --                   | 2025 |
| GUI-R1-7B          | GUI-specific Model (RL)  | --                   | 2025 |
| InfiGUI-R1-3B      | GUI-specific Model (RL)  | --                   | 2025 |
| GUI-G1-3B          | GUI-specific Model (RL)  | --                   | 2025 |
| SE-GUI-3B          | GUI-specific Model (RL)  | --                   | 2025 |
| SE-GUI-7B          | GUI-specific Model (RL)  | --                   | 2025 |
| SE-GUI-G2-7B       | GUI-specific Model (RL)  | --                   | 2025 |


## 4. Challenges & Future Directions

Even with many excellent and innovative algorithms, GUI grounding remains a significant challenge. If anything, the next hurdles resemble high-resolution "boss fights," demanding more sophisticated approaches.

**1. High-Resolution Grounding:** As interfaces become increasingly dense and detailed, achieving accurate grounding at 1080p—or even 4K—requires models to precisely locate elements, some as small as a few pixels. This not only demands higher accuracy but also presents a critical challenge due to the explosion of visual tokens, which leads to substantially higher computational costs and intolerably lower inference speeds. Current Large Vision-Language Models (LVLMs) often downsample aggressively for computational efficiency, inadvertently sacrificing the fine-grained perception necessary for such tasks. Future advancements will necessitate smarter visual compression techniques, multi-scale processing, or progressive zooming mechanisms to preserve critical detail and manage the token burden without prohibitive computational costs.

**2. The Semantic Bridge of Icon Understanding:** A substantial gap persists between text-based grounding (e.g., "click Settings") and icon-based grounding (e.g., recognizing a small ⚙️). Icons are semantically rich but textually silent, conveying meaning through their design, color, and contextual placement. Bridging this semantic gap will likely require multi-stage reasoning that meticulously aligns visual metaphors with linguistic semantics. This could involve leveraging techniques like contrastive icon-text pretraining, compositional visual reasoning, or even incorporating domain-specific knowledge graphs for icon interpretation.

**3. Saturation of Current Benchmarks:** The rapid progress in GUI grounding has led to a noticeable saturation of existing benchmarks. Many current datasets, while valuable, often feature simpler, lower-resolution interfaces or focus on a limited set of common UI elements and actions. As models achieve near-perfect scores on these established benchmarks, their true limitations in complex, real-world scenarios become obscured. The next generation of benchmarks must introduce greater diversity in interface complexity, higher resolutions, a wider array of interaction patterns, and dynamically changing UIs to accurately reflect real-world challenges and genuinely drive innovation forward. This will necessitate datasets with richer annotations, potentially incorporating eye-tracking data or user interaction logs to capture the nuances of human GUI comprehension.


---

**Next: Part III – Computer-Use Agents**

With the rapid saturation of existing grounding benchmarks and the increasing maturity of techniques for training sophisticated multi-turn agents, researchers are shifting their focus towards **end-to-end computer-use agents**. These ambitious systems aim to autonomously handle complete tasks in digital environments without human supervision, implicitly pushing closer to the realization of **Artificial General Intelligence (AGI)** within the **digital realm**.

*(to be continued...)*
</div>
