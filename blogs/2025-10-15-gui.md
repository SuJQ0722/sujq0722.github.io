---
layout: default
permalink: /blogs/2025-10-15-gui/
title: "The Evolving Face of GUI (III) – Computer-Use Agents"
date: 2025-10-15
excerpt: "This post surveys datasets, models, architectures, challenges, tools, and future directions with updated end-to-end GUI agent works through 2025."
tags: [gui, end-to-end training, multi-agent system]
author_profile: false
---

<div class="blog-post" markdown="1">

# The Evolving Face of GUI (II) – Computer-Use Agents

Research on GUI agents has advanced rapidly since the release of early grounding datasets such as the ScreenSpot series, with these benchmarks reaching saturation within a remarkably short time. However, despite the rapid advancement of GUI grounding techniques, current approaches often fail to capture the inherent complexity of real-world GUI interactions, reducing them to oversimplified element grounding tasks. This limitation has prompted a fundamental shift in focus—from grounding individual elements to developing end-to-end agents capable of performing complete GUI-based tasks. In practical scenarios, comprehensive computer-use tasks demand a multifaceted approach that extends far beyond simple grounding, encompassing long-horizon planning, sophisticated action space modeling, and multi-step reasoning. To address these complex requirements, robust systems may incorporate memory modules for context retention across interactions and leverage API integration to enhance operational efficiency, marking an evolution from isolated grounding capabilities to holistic task execution frameworks.

<figure>
	<img src="/blogs/images/cua.png" alt="CUA" />
	<figcaption>Recent development of computer-use agent</figcaption>
</figure>

## Datasets and Benchmarks

In order to train and evaluate robust computer-use agents, the community has built both **open trajectory datasets** (for agent training / imitation) and **benchmarks / evaluation suites** (for measuring task success, generalization, and robustness). Below are expanded lists of each, incorporating the newest works as of 2023–2025.

### Open-source Datasets

Over recent years, several groups have released open datasets of GUI interaction traces (screenshots, states, actions, reasoning traces) across desktop, mobile, and hybrid interfaces. These datasets serve as the raw substrate for training and pretraining agents—from imitation to reinforcement learning.

* **AgentNet (OpenCUA)** — The flagship desktop trajectory dataset in the OpenCUA project. It contains **22.5K human-annotated computer-use tasks** across Windows, macOS, and Ubuntu (12K from Windows, 5K macOS, 5K Ubuntu), spanning 140+ applications and 190 websites. Each trace includes state, action, and chain-of-thought reasoning per step. ([opencua.xlang.ai][1])
* **Computer-Browser-Phone-Use Agent Datasets (Khang-9966 repo)** — A community-curated catalog of datasets used for GUI agents, covering web, mobile, and desktop interactions (click/typing/navigation) with screenshots, element maps, and action logs. ([GitHub][2])
* **GUI-World** — A multimodal GUI-oriented video dataset containing over 12,000 annotated video clips across desktop, mobile, XR contexts. It includes temporally grounded annotations (keyframes, operation history) to assess dynamic GUI understanding. ([GUI-World][3])
* **ScaleCUA** (2025) — A newly proposed dataset scaling open-source GUI agent training; it spans **6 operating systems** and 3 task domains using a closed-loop pipeline of agent synthesis + human correction. It reports strong gains over baselines (e.g. +26.6 on WebArena-Lite, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art open benchmarks. ([arXiv][4])
* **macOSWorld** — Designed specifically for the macOS domain with **202 multilingual interactive tasks** across 30 apps (28 of which are macOS-exclusive). It also includes a safety/deception-testing subset and multilingual instructions in 5 languages. ([arXiv][5])

These datasets help agents learn cross-platform behaviors and multimodal reasoning. The inclusion of multiple OS types, diversity of applications, and chain-of-thought annotations are key trends pushing toward generalist agents.

### Benchmarks & Evaluation Suites

Benchmarks provide standard tasks and metrics (e.g. success rate, step count, robustness) to compare agents. They test not just grounding but full task performance, error recovery, and generalization capability.

* **OSWorld** — A unified real-computer benchmark (Ubuntu, Windows, macOS) with **369 real-world tasks** spanning web apps, file I/O, multi-app workflows, and cross-application logic. The best agents currently achieve ~12.24% success versus ~72.36% for humans. ([arXiv][6])
* **WindowsAgentArena / WAA-V2** — A Windows-native extension of OSWorld, offering ~150+ desktop tasks in real Windows environments. The WAA-V2 version closes evaluation pitfalls in the original. ([arXiv][7])
* **PC-Agent-E benchmark** — As part of the PC Agent-E work, they release a WindowsAgentArena-V2 benchmark and show cross-OS generalization to OSWorld. ([arXiv][7])
* **ScreenSuite** — A recent evaluation suite that stitches together **13 GUI benchmarks** to test capacities from grounding to multi-turn agentic behavior. It supports launching various OS sandbox environments (Ubuntu, Android) in Docker. ([Hugging Face][8])
* **OSUniverse** — A newer desktop-oriented benchmark focused on GUI navigation and workflows. It reports new results building on OSWorld’s baseline. ([arXiv][9])
* **macOSWorld evaluation** — While primarily a dataset, it doubles as a benchmark for agents’ performance on macOS in multiple languages and under deception attacks. ([arXiv][5])

Together, these benchmarks let us compare agent performance across OS domains, task complexity, multilingual settings, and safety scenarios. They reveal persistent gaps and challenging failure modes in current agents.

---

## E2E Training

With the advent of multimodal large language models (MLLMs) and agentic reasoning frameworks like ReAct, many works now exploit an LLM’s visual perception and planning capabilities to build computer-use agents. Early pioneers include **OpenAI’s Computer-Using Agent (CUA)** and **Anthropic’s Claude agent developments**. In open-source terrain, **UI-TARS** and others have led the push for fully transparent agent models.

Below is an overview of representative E2E agent systems and model families (2023–2025), along with their training strategies and architectural contributions.

| Model / System                                                        | Highlights & Approach                                                                                                                                                                                                                                    | Key Strengths / Tradeoffs                                                                                                  |
| --------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| **OS-Atlas**                                                          | A foundational GUI behavior model trained via SFT on the largest open cross-platform GUI grounding dataset to date (13M+ annotated elements). Combines multiple existing datasets with proprietary annotations, emphasizing grounding and generalization across mobile, desktop, and web interfaces. | Powerful open-source baseline excelling at GUI localization and zero-shot generalization to unseen interfaces; strong performance across diverse benchmarks |
| **PC Agent-E**                                                        | Uses only 312 human trajectories + Trajectory Boost (LLM-assisted augmentation) to train an efficient agent that outperforms stronger baselines on WindowsAgentArena-V2 and generalizes to OSWorld. ([arXiv][7])                                         | Highly data-efficient; demonstrates that small but high-quality trajectory datasets + augmentation can yield strong agents |
| **GUI-Owl (Mobile-Agent-v3)**                                         | GUI-Owl serves as the foundational model supporting multiple OS, then the framework “Mobile-Agent-v3” iteratively self-evolves its data through simulated exploration and refinement. Achieves 73.3% on AndroidWorld and 37.7% on OSWorld. ([arXiv][12]) | Exemplifies true cross-platform generalism; good tradeoff between performance and openness                                 |
| **ScaleCUA**                                                          | Focuses on scaling data: trains on a dataset spanning six OSes, across domains. Demonstrates strong gains on WebArena-Lite, ScreenSpot-Pro, and OSWorld-G. ([arXiv][4])                                                                                  | Emphasizes scale and coverage, facilitating improved agent robustness         |                                             |
| **UI-TARS / UI-TARS2**                                                | A core open-source GUI agent family designed with unified modeling across OS types, using multimodal reasoning + action planning (CPT + SFT + RL). (Mentioned in your baseline list; widely used as benchmark)                                                 | Community-leading open agent architecture; acts as reusable baseline                                                       |


**Training techniques & trends:**

* Many recent works adopt a pipeline of **supervised fine-tuning (SFT) → preference tuning / DPO → RL or self-improvement**, blending imitation and reward-based learning.
* **Trajectory augmentation** (as in PC Agent-E) is increasingly popular: using LLMs to expand or diversify human trajectories before training.
* Joint training for **grounding + reasoning** modules (rather than separate isolated modules) is gaining traction.
* **Self-improving loops** (agent executes, reviews, refines its own data) are a rising paradigm (Mobile-Agent-v3 uses this).
* Models often treat the **screen + mouse/keyboard interface** as universal, making the same core agent usable on both mobile and desktop.

---

## Multi-Agent Architectures

Relying on a single monolithic model for all GUI capabilities can limit specialization and adaptability as tasks vary wildly (e.g. form-filling vs. file management vs. app-specific flows). To address this, several recent works decompose agents into collaborating submodules or specialists.

* **Agent S2** exemplifies this: it uses a **generalist planner** plus **specialist grounding / reasoning modules**, with a custom Mixture-of-Grounding that dispatches to the best specialist per UI context. ([arXiv][11])
* **UFO / Desktop AgentOS (Microsoft)** employs a **HostAgent + AppAgents** architecture: HostAgent decomposes tasks, and AppAgents (per app) handle execution via vision + native UI Automation. This hybrid approach improves reliability and error recovery.
* **OpenAI Agents SDK / AgentKit / Swarm-style orchestration** allows chaining or orchestrating multiple agents (e.g. one module does vision, another planning, a third state cleanup). Though not GUI-specific, it enables building multi-agent GUI workflows.
* **Module-based pipelines in UI agent projects**: many models internally split perception, grounding, plan generation, and action modules — though not always as independent agents, the conceptual modularization helps debugging, scaling, and flexibility.

Modular and multi-agent designs improve flexibility and robustness. For instance, if a grounding module fails, a fallback specialist can intervene; or a planning module can be replaced or updated without retraining perception.

---

## Hybrid Architectures: Combining End-to-End RL with Multi-Agent Systems

Though multi-agent frameworks harness context engineering, role separation, and task decomposition, their cascaded or pipelined structure can subtly accumulate errors and constrict the performance ceiling of the underlying model. In those designs, mistakes in grounding or planning propagate downstream agents, amplifying deviations. This limitation has motivated hybrid architectures that combine an end-to-end, GUI-trained foundational model with multi-agent orchestration for optimization, error correction, and task decomposition.

One prominent example is Mobile-Agent-v3([arXiv][12]), which uses GUI-Owl as its foundational backbone and wraps a multi-agent system (MAS) around it for enhanced control and robustness. GUI-Owl is trained as an end-to-end multimodal agent (vision → grounding → planning → action), capable of operating directly on desktop and mobile interfaces. Then Mobile-Agent-v3 orchestrates specialized agents (e.g. Manager, Worker, Reflector, Notetaker) to supervise, reflect, and guide the execution of long-horizon tasks using dynamic replanning, memory, and error recovery. This hybrid approach mitigates cascading error drift while retaining the capacity and flexibility of a single, learned core model.

---

##  Cross-Platform Approaches

Achieving **universality** — i.e. one agent that works well across desktop, mobile, and web — is a key ambition. Hybrid strategies help bridge domain gaps and make cross-platform generalization possible.

* **Unified interface abstraction**: Many agents (CUA, PC Agent-E, UI-TARS) treat the GUI as a rendering + mouse/keyboard action space, hiding OS-specific APIs under a universal interface. This allows the same core agent to act on both Android, Windows, macOS, and web. (CUA is an example: uses screenshot + keyboard/mouse regardless of OS) ([OpenAI][10])
* **Hybrid control (vision + native APIs)**: For example, Microsoft’s UFO leverages native UI Automation where possible and falls back to vision-based control otherwise. This increases robustness and reduces brittleness.
* **Cross-OS trajectory synthesis / self-evolution**: Mobile-Agent-v3’s training pipeline runs agents in Android, Ubuntu, Windows, macOS environments in cloud infrastructure, generating trajectories that span OS types. ([arXiv][12])
* **ScaleCUA’s cross-platform dataset**: By training agents on diverse operating systems (six OSes) and domains, it encourages models that generalize across devices. ([arXiv][4])
* **macOSWorld**: Fills a gap by explicitly focusing on macOS GUI tasks (including multilingual and safety scenarios), helping to adapt agents to that domain. ([arXiv][5])

By combining shared modeling, API fallback, cross-platform data, and modular interfaces, hybrid architectures approach the goal of a general GUI agent.

---

## Emerging Tools, Frameworks & Platforms

Beyond research papers, a growing ecosystem of tools, SDKs, and open frameworks is enabling broader experimentation in GUI agents.

* **OpenAI Computer-Using Agent (CUA) / Operator** — A high-profile agent preview. It treats GUI control via universal screenshot + mouse/keyboard interfaces, achieves 38.1% on OSWorld and 58.1% on WebArena, and emphasizes safety (asking for confirmation for sensitive actions). ([OpenAI][10])
* **AgentKit / OpenAI Agents SDK / Swarm tools** — Toolkits for composing agents, chaining workflows, and integrating external tools. These can be repurposed to orchestrate GUI modules (vision, planning, execution).
* **PC Agent-E open-source suite** — The PC Agent-E project publishes its code, dataset, and models for reproducibility and extension. ([arXiv][7])
* **ScaleCUA’s release** — The dataset, models, code for the cross-platform agent are intended for public release to support future GUI agent work. ([arXiv][4])
* **macOSWorld release** — Providing open benchmark, multilingual tasks, and safety tests for GUI agents targeting macOS. ([arXiv][5])
* **ScreenSuite** — An evaluation suite that aggregates multiple GUI benchmarks and supports launching sandbox OS environments via Docker for evaluation. ([Hugging Face][8])
* **Community repositories (e.g. trycua / acu, GUI agents resource lists)** — Collections of papers, tools, frameworks, benchmarks for GUI agent research. ([GitHub][13])

These tools reduce the barrier to entry, foster reproducibility, and allow researchers to experiment with agent architectures without building full infrastructures from scratch.

---

## Challenges in Current Research

Even with all these advances, computer-use agent research faces several persistent and emerging challenges. Below I elaborate each with concrete examples and recent mitigation techniques.

### Generalization to Unseen Interfaces

Agents often overfit to UI layouts or app families seen in training. When presented with new designs or apps, they fail to adapt.

* Mitigation: Use **diverse cross-platform datasets** (ScaleCUA, AgentNet, GUI-World), and **data augmentation / LLM-based trajectory blending** (PC Agent-E’s Trajectory Boost).
* Open problem: true zero-shot adaptation to wholly new domain types (e.g. VR apps, games, novel UI style languages).

### Long-Horizon Planning & Sparse Rewards

Complex tasks (e.g. “organize files, then email results”) require tens of steps and delayed feedback.

* Mitigation: **Hierarchical planning**, **semi-online RL** (UI-S1 style), **reward shaping**, **subgoal decomposition**.
* Open problem: maintaining robustness and recovery in long sequences with branching branches.

### Safety, Robustness & Error Recovery

Agents interacting with GUI risk performing unsafe or destructive actions (e.g. closing important documents, clicking malicious popups).

* Mitigation: **Action masking**, **human-in-the-loop confirmation**, **state rollback / undo modules**, **safety benchmarks** (macOSWorld includes deception tests).
* Open problem: guaranteeing safety in open environments with adversarial UIs or unseen popups.

### Efficiency, Memory & On-Device Inference

Running giant models is expensive, particularly on mobile or desktop inferences.

* Mitigation: **Quantization**, **smaller models** (AgentCPM, 8B), **vector memory modules** (e.g. UFO’s memory), **caching embeddings**, **state compression**.
* Open problem: balancing model capacity vs latency vs reliability in real-world GUI tasks.

### Proactivity and Autonomy

Most agents are reactive: they wait for instructions and then act. Proactive agent behaviors (anticipating user needs, automating routine flows) is barely explored.

* Mitigation: Some modular frameworks (AgentKit, Agent S2) allow agent chaining or planning ahead; **feedback loops** and **user profiling modules** may help.
* Open problem: trust, interpretability, autonomy governance — how to let the agent act without unwanted surprises.

---


[1]: https://opencua.xlang.ai/?utm_source=chatgpt.com "OpenCUA: Open Foundations for Computer-Use Agents"
[2]: https://github.com/Khang-9966/Computer-Browser-Phone-Use-Agent-Datasets?utm_source=chatgpt.com "Khang-9966/Computer-Browser-Phone-Use-Agent-Datasets"
[3]: https://gui-world.github.io/?utm_source=chatgpt.com "GUI-World: A Dataset for GUI-Oriented Multimodal LLM-based Agents"
[4]: https://arxiv.org/abs/2509.15221?utm_source=chatgpt.com "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data"
[5]: https://arxiv.org/abs/2506.04135?utm_source=chatgpt.com "macOSWorld: A Multilingual Interactive Benchmark for GUI Agents"
[6]: https://arxiv.org/abs/2404.07972?utm_source=chatgpt.com "OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks ..."
[7]: https://arxiv.org/html/2505.13909v1?utm_source=chatgpt.com "Efficient Agent Training for Computer Use"
[8]: https://huggingface.co/blog/screensuite?utm_source=chatgpt.com "ScreenSuite - The most comprehensive evaluation suite for GUI ..."
[9]: https://arxiv.org/html/2505.03570v1?utm_source=chatgpt.com "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents"
[10]: https://openai.com/index/computer-using-agent/?utm_source=chatgpt.com "Computer-Using Agent | OpenAI"
[11]: https://arxiv.org/abs/2504.00906?utm_source=chatgpt.com "Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents"
[12]: https://arxiv.org/abs/2508.15144?utm_source=chatgpt.com "Mobile-Agent-v3: Foundamental Agents for GUI Automation"
[13]: https://github.com/trycua/acu?utm_source=chatgpt.com "trycua/acu: A curated list of resources about AI agents for ..."
 

</div>